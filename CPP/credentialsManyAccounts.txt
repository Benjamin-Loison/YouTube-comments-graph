Youtube Data
CENSORED
CENSORED
YouTube Data API v3

CENSORED
CENSORED
CENSORED
CENSORED

https://console.cloud.google.com/
https://console.cloud.google.com/marketplace/product/google/youtube.googleapis.com?q=search&referrer=search

https://youtube.googleapis.com/youtube/v3/channels?part=contentDetails&id=UCzYC9ss2P77Ry2LzIDL5Xsw&key=YOUR_API_KEY

well it stops really fast any account creation

when reset quota ? unclear

make an algorithm to check how many credits this day on the key (and spending them so)
how to check them correctly ? could check all, spend one and check that it was alone to be spent
can use this to do it faster https://developers.google.com/youtube/v3/docs/search/list
can use multithreading
or just use simple get costing one likewise can see precisely
could check them one day see if someone lied

check at what time reset - "done"

some comments number are absolutely not correct - patched

add upload time to comments details - done

could compress details by making table of matching uuid to youtube uuid, use a short name or none for the youtuber, write just the author of comment thread if any, compress date format and if really need zip file until being used

https://developers.google.com/youtube/v3/docs/search/list
order date could allow to check all video being uploaded in live

https://youtube.googleapis.com/youtube/v3/search?key=YOUR_API_KEY&maxResults=50&type=video
returns the limit of 1 000 000 but can restrict by time and likewise get them all

regionCode,relevanceLanguage
https://youtube.googleapis.com/youtube/v3/search?key=YOUR_API_KEY&maxResults=50&type=channel&order=viewCount&relevanceLanguage=fr

-------------------------

why 0 comments ? - no problem on main algorithm because get comments of channels directly
The Voice : la plus belle voix
c'est la seul qui semble avoir ce problème
https://www.youtube.com/playlist?list=UUQRELbX0H5FCokIFxOAsHFA - playlist non visible depuis le web mais avec l'API aucun soucis u_u
https://www.googleapis.com/youtube/v3/playlistItems?part=contentDetails&maxResults=50&playlistId=UUQRELbX0H5FCokIFxOAsHFA&key=YOUR_API_KEY
TODO: /!\ /!\ /!\ /!\ /!\ /!\ /!\ /!\ /!\ /!\ /!\ /!\ /!\ /!\ /!\ /!\
TODO: also make an option to multithread a single channel (for the big one for instance) because we have a lot of keys because there are 23 (11* when not having put credit card to validate account) projects and so key which the quotas don't overlap per google account
we have to activate for each project the Youtube Data API v3

https://www.googleapis.com/youtube/v3/search?key={your_key_here}&channelId={channel_id_here}&part=snippet,id&order=date&maxResults=20

why can't find my name in squeezie comments-names ?

en monothread en moyenne sur 15h19 2.03 requêtes par seconde

let see what search channels of youtube gives as output (do I find myself and CENSORED for instance and top 100 france) otherwise we'll have to do the discovery my commenterId
TODO: check if really french channel

WARNING SOME CHANNELS LIKE MY SECONDARY ONE DOESN'T HAVE A COUNTRY assigned !

need to activate keys it seems - done

not running discovery for the moment just to get top 100 fr to see what size it is - squeezie isn't a good "example"

papa pig really 0 comment and my algorithm thread doesn't seem to finish on it 

my current question is: do peppa pig on commentThreads (which only work for a channel or a single video) error means error on ALL videos or just some ?
- well 3F8dFt8LsXY gives the answer and it is yes, (can't list channels videos comments but can on some videos)
- let's not take into account these kind of channels but we write it somewhere likewise we can consider them later

could also manage modified comment ? cf la chanson des commentaires 0:52

according to https://developers.google.com/youtube/v3/guides/implementation/comments: need to use https://developers.google.com/youtube/v3/docs/comments/list to get all replies to a top-level comment - should do some tests!

might seem ridiculous but 100 threads is not that big because of each one waiting for ping and a few are dropped quite fastly because of peppa pig like
-> should multithread for a single channel ^^

maitre gims n'est pas français
-> ne traitons que les français déclarés sinon je vais traiter un coréen qui n'est pas déclaré coréen
--> TODO: should auto detect another way whether or not the youtuber is french

top100fr (il y a 2 chaînes maîtres gims en plus :'()
13 peppa pig
26 pas déclaré français
38 en ||

could work by location but that only list video which precise the recording location
no more info in channels/videos list - can only try to get language from video or descriptions

bonnietylerVEVO in top 100 fr u_u according to socialblade - socialblague ouais
little channel like https://www.youtube.com/c/Ertinox12/about doesn't have any country assigned by socialblade so they haven't hidden tools i think

--------------------

make algorithm to manage also peppa pig channel

check if full french
can get country from "A propos" (could also work on description of most viewed video for instance or everyone) of the channel, links:
- email address
- website
- facebook
- instagram
- twitter
- a secondary youtube channel
- twitch
- tiktok

--------------------------------

Ce qu’il a DÉCOUVERT sur la ZLAN grâce à la data science ! (c'est ouf)
https://www.youtube.com/watch?v=N61_kHXpaFA

https://youtu.be/N61_kHXpaFA?t=226 pas le cas pour YouTube easy pour voir vidéos

où est Micode

fragmenter data set que pour voir audience de micode par exemple
https://youtu.be/N61_kHXpaFA?t=411

cartographie ciblé sur le top 100 fr (il a fait)

étapes:

1. récupérer données par API - partie la plus dur pour les autres plateformes que Twitch - fais du multithreading pour accélérer
2. datavisualisation https://youtu.be/N61_kHXpaFA?t=628 gephi logiciel https://github.com/gephi/gephi
PNG 45 000x45 000
2 millions de personnes représentés
2 000 streamers

une demi heure de scrap 1 million de lignes sur excel

data visualisation
méthodologie et couleurs (pas de vert/rouge)
taille du noeud dépend du degré chez lui ou dégré pondéré en fonction du nombre par utilisateur si pondéré ça peut faire paté et c'est pas fou (il n'a pas fait ça)

moi: faire que à un pseudo donné voit où est sur le graphe

fais des représentation en fonction du temps: le matin, tel jour (ordre de grandeur pas fou dans mon cas)
could sort by topic "topicDetails" to make communities ? (always given automatically ?)

https://www.youtube.com/channel/UCMQSwUNSnMOP4IRysfeg8EA est intéressé de dingue par
tweeter (il l'a déjà fait le mec: tweeter fr sur une journée, tweet ayant fait au moins une réponse||un like||un retweet, on voit les mentiosn graphiquement,
il y a le congo mdr, scraping car API ne donnent que 15 tweets toutes les 15 min), youtube, insta
intérêt particulier pour YouTube de Simon Puech
le mec répond "youtube bloque tout, mais il y a quand même des moyens" (niveau robinet à data des gafam) - j'ai envie de dire que non à part pour les vues
pas illégal mais non respect des CGU etc donne l'exemple de scraper des commentaires d'une vidéo récupérant que les pseudos comme si on les notait à la main
micode répond "ça ne vaut pas autant que des vues mais potentiellement ça pourrait être 
Simon Puech répond que les commentaires sont peu représentatif https://youtu.be/N61_kHXpaFA?t=1740 aimerait faire avec les vues
Micode chaud pour avoir l'algorithmes YouTube (quelles vidéos il va te proposer etc) - suggestions pas dispo à part pour nos propres vidéos les bgs peut être intéressant malgré tout ?

-------------

at least

YOUR_API_KEY seems to be suspended

make a tool to count comments retrieved (to compare with theoric one from statistics)

were suspended (and so removed from keys and accounts): why ?

could make an auto tool to remove them and list them (and give indice in a mail list)

---------------

if this project comes to a happy ending one day, check my own comments before releasing data ^^

------------------------

Twitter:

API Key: CENSORED
API Secret Key: CENSORED
Bearer Token: CENSORED


CENSORED
CENSORED
CENSORED

t whois @minette2400

------------------------------------------

test if more than 100 answers what happen with maxResults (shouldn't be a problem) ? - if more than 5 need to use comments list and if more than 100 need pagination to use

searchTerms cool for testing on big channel ? not required it seems for the moment

could log comment id ? done

what was the youtube document thing saying use other API please ? https://developers.google.com/youtube/v3/docs/commentThreads?hl=fr "The commentThread resource does not necessarily contain all replies to a comment, and you need to use the comments.list method if you want to retrieve all replies for a particular comment."

statistics commentsNumber != in reality example 5g0j29G2w58 88 on API and displayed but if we count (done more than 3 times) we always got 84 and my algorithm also got that while looping into comments videos thourgh API
well the algorithm seem to be right now (because of 5 max answers) but by hand seems to still have trouble let's compare
no it was by hand an error because not seeing all because "top comments" by default

prefixes:

UC for channel
Ug for comments (also seem to have a suffix ?)

ok commentThreads only five first answers to comments
if want them all have to provide with parentId to comments and use pagination

parentId: Note: YouTube currently supports replies only for top-level comments. However, replies to replies may be supported in the future.

--------------------

/usr/local/bin/t: execution expired
getaddrinfo(whois.verisign-grs.com): Temporary failure in name resolution
getaddrinfo(whois.verisign-grs.com): Temporary failure in name resolution
getaddrinfo(whois.verisign-grs.com): Temporary failure in name resolution
/usr/local/bin/t: execution expired
getaddrinfo(whois.verisign-grs.com): Temporary failure in name resolution
getaddrinfo(whois.verisign-grs.com): Temporary failure in name resolution
getaddrinfo(whois.verisign-grs.com): Temporary failure in name resolution
getaddrinfo(whois.verisign-grs.com): Temporary failure in name resolution
ERROR: Unable to download JSON metadata: <urlopen error [Errno -3] Temporary failure in name resolution> (caused by URLError(gaierror(-3, 'Temporary failure in name resolution'),))
getaddrinfo(whois.verisign-grs.com): Temporary failure in name resolution
terminate called after throwing an instance of 'nlohmann::detail::parse_error'
  what():  [json.exception.parse_error.101] parse error at line 1, column 1: syntax error while parsing value - unexpected end of input; expected '[', '{', or a literal
51      ../sysdeps/unix/sysv/linux/raise.c: No such file or directory.
ERROR: Unable to download JSON metadata: <urlopen error [Errno -3] Temporary failure in name resolution> (caused by URLError(gaierror(-3, 'Temporary failure in name resolution'),))
ERROR: Unable to download webpage: <urlopen error [Errno -3] Temporary failure in name resolution> (caused by URLError(gaierror(-3, 'Temporary failure in name resolution'),))
ERROR: Unable to download JSON metadata: <urlopen error [Errno -3] Temporary failure in name resolution> (caused by URLError(gaierror(-3, 'Temporary failure in name resolution'),))
ERROR: Unable to download JSON metadata: <urlopen error [Errno -3] Temporary failure in name resolution> (caused by URLError(gaierror(-3, 'Temporary failure in name resolution'),))

----------------

https://www.facebook.com/pages/Les-fans-de-Bob-Lennon/131382033594694
->
https://www.facebook.com/profile.php?id=100044581142720
also works just with all pages ids

videos statistics shows comments number strictly positive while the videos comments are disabled u_u
and if really want to know how many comments there are on a given youtube channel have to get them all, results info aren't accurate (limit to maxResults)

https://www.youtube.com/watch?v=3F8dFt8LsXY is the only comment allowed video from the whole channel (peppa pig like)

------------------------------

checking if squeezie don't lose anymore the 2 % of comments

amixem check comments chanson

treat CENSORED case - langdetect with all description of channel videos do the job - but we have to think if one test is ok do we really say it's french ? :S

begin coding discovery system - wait squeezie and amixem tests

get more API keys (father) - done

it would also be interesting to be able to multi computer (first time that i would do that btw) - well need more keys because otherwise i got my laptop, oc3k, ovh vps and father laptop
-> like my named pipes database for LemnosLife, would need tcp network make master/slave my main computer as master (with a new program managing) and (also slave)
-> master first have something like top 100 fr ids, split by computer number and each one at launch ask job to do (channels to check if french and if so work on) - result on slaves aren't automatically transfered likewise can zip later etc to optimize the big data transfer - well that's not possible because we want names of new channels to work on, could automatically send new channels met by making a difference when got this whole list (by requesting the already done difference to master)
-> should just pay attention if slave disconnect should reput in wait queue its task

-----------------

if still having curl problems should look for this use OpenSSL version

https://curl.se/libcurl/c/threaded-ssl.html
https://curl.se/libcurl/c/threadsafe.html

is it a problem to make multiple system calls to curl ? - it doesn't seem according to internet

--------------------------

make alternative to getVideosYTDL using the API for TheVoice and also to force if problem with playlist videos ids download with youtube-dl - done
because got that: ERROR: Unable to download JSON metadata: <urlopen error [Errno -3] Temporary failure in name resolution> (caused by URLError(gaierror(-3, 'Temporary failure in name resolution'),))

and should make a thread analyzer

QUOI QUE OUBLIONS LE MULTITHREADING (problème de synchronisation niveau édition du fichier details par exemple je pense, mieux vaut opter pour un multithreading sur les chaînes youtubes et non les vidéos)
MAIS GERONS LE CAS THE VOICE PROPREMENT - should be ok - to test - well in fact it's not peppa pig so don't need it even if now it's coded

----------------------------------

lire eula compte google pour cloud plateform etc ?

if video peppa pig show videoId too not just index progress - done
add to treated costConsumed/web requests - done

dans le pire des cas avec la chaîne de Squeezie il faudrait 11.2 jours pour la traiter entièrement si on était dans le pire cas de tous les commentaires on juste 5 réponses
sinon ça prendrait 13h en monothread sur un chaîne u_u merci youtube
instead of theorically 22 fois plus long en pratique on dirait que 8 fois

a question after having done squeezie is how many such requests just to have answers ?

---------

maybe creating a basic random french account and link videos by suggestions each one to each other may give an interesting graph if after only showing youtuber names etc

one public comments the other disabled (maybe directly) should find how youtube deal with a video not directly comments disabled (with comments before)

https://youtube.googleapis.com/youtube/v3/videos?part=statistics,status&id=zQW07uf6CLk&key=YOUR_API_KEY
https://youtube.googleapis.com/youtube/v3/videos?part=statistics,status&id=Tw8mpgccugc&key=YOUR_API_KEY

----------------------

furious jumper also desactivated channel comments retrieve thourhg commentThreads list channelId (and so allThreadsRelatedToChannelId it seems)
could grab them by hand ? - il semble que le scroll et ajout de contenu se fasse dans un truc JavaScript sans utilisation de choses détectables avec l'outil réseau du navigateur ^^'
pourrait ne pas considérer les commentaires de chaînes dans les graphes finaux

furious jumper can have 4 940 875 comments according to realNumberOfComments.py (with not counting comments disabled videos)
and my main.cpp in CommentsCounter say 4 940 912 (with counting comments disabled videos)
0 commentaires sur les quelques vidéos où ceux-ci sont désactivés

could make a system to add while running some keys

je ne sais pas trop mais il semble y avoir un anti-get avec l'API YouTube sur les commentaires sur la chaîne (et non les vidéos) de JUL: 
https://youtube.googleapis.com/youtube/v3/commentThreads?part=snippet,replies&channelId=UCdeIkFMrH0N_feYzyFcIBUw&key=YOUR_API_KEY retourne bien quelque chose contrairement à
https://youtube.googleapis.com/youtube/v3/commentThreads?part=snippet,replies&channelId=UC-GI5LST5T3Gw93yZxjdFaw&key=YOUR_API_KEY

sinon on a 17 000 commentaires en pratique de plus qu'une prédiction (en utilisant le nombre de commentaires déclarés sur chaque vidéo et en vérifiant que l'on a bien accès à au moins l'un) effectuée après récolte je pense que les compteurs de YouTube à commentaires ne sont pas mis à jour en temps réel
coded checkClaimedNumberOfCommentsAndReal.py to see if it's the case

-------------------------------

seems that suspended keys let one of each account ok so can abuse temporarily maybe it is worth, at least we don't lose anything except time

what is the french most commented video ? (and how manu comments does it have ?)

la seule amélioration à faire avant d'avoir le top 100 fr serait de multi computorisé tout ça et faire que curl ne fasse pas appel au shell pour accélérer le processus, actuellement les requêtes de squeezie mettent 2.2 s à s'effectuer, en espérant qu'en finissant le reste du top 100 ça améliore la situation (si le top100 restant n'est pas dans le même cas)
et faire que le maxCommentedChannelDone.py détaille l'avancement de chaque chaîne dans son traitement - unless making approximation need to restart the algorithm to have such precise info ? oh if reading from details file can guess what is current comments number

Could check video per video why our comments counter exceeds real one
- doesn't seem to be required

ça a l'air compliqué de compter jusqu'à 50 pour youtube (bon on va dire qu'ils ont ban un compte et me réponde donc pas sur l'un) https://youtube.googleapis.com/youtube/v3/channels?part=snippet&id=UC1vAVsfalm1AqZSgjMaIlZg,UC1vAXlff6m_4GMI3EfWgEPA,UC1vAd3fBEszA3QdV5Xve4Ew,UC1vAp2sl5mwXMkA5s1WJbTw,UC1vAzdOteMPztO5jsxwqEyQ,UC1vB9TlzM0-_1rsLVXblM-g,UC1vBKnrFppsrVCdUqWLM1MQ,UC1vBUw2HlRoAwqY4p048kow,UC1vBZlf20ipGiokQbUfCgAQ,UC1vBlyGjVwmhVXj8OtSSqww,UC1vBqDK8x0snqnJ6CglBZfg,UC1vC-JZvV51y5Fd8znz6diA,UC1vC-e7QLFD1WzoeG1gY6wA,UC1vCGLyKcT6MiB5zErgscJw,UC1vCTujY4Jq66dlII6Jag7w,UC1vCU0FLZk839zLRpu248PQ,UC1vCiP8xyCMwm6HIm3xqLnA,UC1vCz3j1XfQoCajrCfC6k6g,UC1vD2-iqFxqZy10qEWfhlTQ,UC1vDIwheltYJUY2CeMdAt7Q,UC1vDKyrZ862MKBOIdQK7bYA,UC1vDVfU5dFKw_DlX3b3BNAA,UC1vDnNtE9FNiCrpDmzboU2g,UC1vE2jKediI-ujCw1HcWuKw,UC1vEh4YHfr-txVE0_8uWazQ,UC1vEhrO1yO_olND14_0ZsSg,UC1vF9VrJtbilHVkiX8TJxaQ,UC1vG8_Pb6pmrwX6pLFDKbuA,UC1vGGwbSydLKpsA2CDTb7Ew,UC1vGKrFK0wzZFtxSJOXaVgA,UC1vGR8zLqF83yqu143PwnRQ,UC1vGbiSgKoIYNJ9EUE_6UEA,UC1vGe49ne6dqyIwK73zkTzA,UC1vGi4zQDNwssFH8Jn6x4Ig,UC1vGtaDE5E6cdK4NL8kQ4yA,UC1vH8WE2jPXxdO6ca21-fUw,UC1vHGpgoecB7Rdav-98qTPA,UC1vHNqJI1xuxNM9hnqtymqA,UC1vHPF3BHaJ1hN18suu0BZw,UC1vHgTc1JPv-_WynWjzO7Hw,UC1vHoTud1gbeTtPwhtN2CWA,UC1vI-a8CfKEeenJLhTNyQaA,UC1vIDQx5AJ9SEBJk0WHzWWA,UC1vIZwpaptimrtcSH3bNNBA,UC1vIaK8DoTSfYyZI0DiRGuQ,UC1vIgrev9bBUcQ-LPDvYumg,UC1vJ8TaJwL5Wh_1pa105SOA,UC1vJ9j7r7Jx9o_Au-l2-JLA,UC1vJTooYG2JdPcOk8aqyVSg,UC1vJUX1Lfsp6hjtpaKvMMZA&key=YOUR_API_KEY

have to escape ' for curl command ? - yes

make a code backup before executing program otherwise lost in lines numbers

----------------

allez un nouveau bug de youtube, bon alors pas important pour l'algo en lui même mais pour les statistiques de vérification, je me base principalement sur la playlist qui est auto généré par youtube listant toutes les vidéos d'une chaîne et dans le cas de chaîne uploadant plus de 20 000 vidéos la playlist n'est pas exhaustive: https://www.youtube.com/playlist?list=UUJldRgT_D7Am-ErRHQZ90uw https://socialblade.com/youtube/channel/UCJldRgT_D7Am-ErRHQZ90uw

when asking search order=date the dates within a call can be not ordered perfectly

not answering and not patching... https://issuetracker.google.com/issues/198313889
https://issuetracker.google.com/issues/199176593
vraiment des potes à nous dire ouais ouais et ils disent la même depuis 2 ans
-> https://issuetracker.google.com/issues/166292064 "it's not a bug"

---------------------

show curve speed squeezie

and check if got top 100 as predicted

----------------------

thevoicefr pb playlist
pb "france inter", "france 24", "euronews", "france 24 arabic" limit playlist 20 000 - look for 400 euronews is in this case too

- after searching on youtube https://www.youtube.com/results?search_query=euronews&sp=EgIQAg%253D%253D
france inter 24 770 uploads - 24 784
france24 85 313 uploads - 85 354
euronews (en français) 96 481 uploads - 96 481
france24 arabic 102 568 uploads - 102 602

total: 294 521
need 59 keys

not easy to browse videos with curl, could use selenium instead maybe but maybe too heavy ?
it is too slow

let's use search api :'(

-----------

search api doesn't work correctly too, let's try to rework on my curls requests to simulate a video browsing
top 100 yt fr: 19 457 798 commentateurs pour 64 487 685 de relations différentes donc 3.3 en moyenne

Studio Bubble Tea: 113.8: 55017
Juste pour rire les gags: 99.95: 45386
Alonzo: 99.98: 36443
We Are Kids United: 99.57: 31544
TeeTee TV: 98.5: 9254
Black M: 98.98: 7076
Osratouna tv - قناة أسرتن&hellip;: 100.0: 31

------------------------------------

